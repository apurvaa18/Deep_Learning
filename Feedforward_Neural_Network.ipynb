{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Design a feedforward neural network from scratch for following neural network.\n",
        "A) Binary Classication\n",
        "B)Multiclass Classication(4 input Neuron, 3 neuron in rst hidden layer, 4 neuron in second hidden layer,3 neuron in output layer)\n",
        "\n",
        "Generate dataset using make_blobs,make_moons,make_circles. Use sigmoid activation function for all layer in Binary Classication. Sigmoid at\n",
        "hidden layer and softmax at output layer for multiclass classication. Use mean squared error loss function for binary classication and binary\n",
        "cross entropy loss function for multiclass classication."
      ],
      "metadata": {
        "id": "Pm2cmQinlGrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) Binary Classication"
      ],
      "metadata": {
        "id": "M7fm4v6Lk5we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "egxO5SrObb32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        " return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_derivative(x):  #\n",
        " return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Softmax activation function\n",
        "def softmax(x): #multiclass\n",
        " exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        " return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Loss functions\n",
        "def mean_squared_error(y_true, y_pred):\n",
        " return np.mean((y_true - y_pred) ** 2)\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        " return -np.mean(y_true * np.log(y_pred + 1e-9))"
      ],
      "metadata": {
        "id": "Q8Er8i0Sbhsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedforward Neural Network class\n",
        "class FeedforwardNeuralNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, binary_classification=True):\n",
        "        self.binary_classification = binary_classification\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(input_dim, hidden_dim)\n",
        "        self.b1 = np.zeros((1, hidden_dim))\n",
        "        self.W2 = np.random.randn(hidden_dim, output_dim)\n",
        "        self.b2 = np.zeros((1, output_dim))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = sigmoid(self.z1)\n",
        "\n",
        "        # Hidden to output layer\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        if self.binary_classification:\n",
        "            self.a2 = sigmoid(self.z2)  # Sigmoid for binary\n",
        "        else:\n",
        "            self.a2 = softmax(self.z2)  # Softmax for multiclass\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, y_pred, lr):\n",
        "        # Output layer error\n",
        "        if self.binary_classification:\n",
        "            d_z2 = (y_pred - y) * sigmoid_derivative(self.z2)\n",
        "        else:\n",
        "            d_z2 = y_pred - y\n",
        "        d_W2 = np.dot(self.a1.T, d_z2)\n",
        "        d_b2 = np.sum(d_z2, axis=0, keepdims=True)\n",
        "\n",
        "        # Hidden layer error\n",
        "        d_a1 = np.dot(d_z2, self.W2.T)\n",
        "        d_z1 = d_a1 * sigmoid_derivative(self.z1)\n",
        "        d_W1 = np.dot(X.T, d_z1)\n",
        "        d_b1 = np.sum(d_z1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.W2 -= lr * d_W2\n",
        "        self.b2 -= lr * d_b2\n",
        "        self.W1 -= lr * d_W1\n",
        "        self.b1 -= lr * d_b1\n",
        "\n",
        "    def train(self, X, y, epochs, lr): # Define the train method within the class\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            if self.binary_classification:\n",
        "                loss = mean_squared_error(y, y_pred)\n",
        "            else:\n",
        "                loss = binary_cross_entropy(y, y_pred)\n",
        "            self.backward(X, y, y_pred, lr)\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "V9bNkyvsdy5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train binary classification model\n",
        "binary_model = FeedforwardNeuralNetwork(input_dim=2, hidden_dim=5, output_dim=1, binary_classification=True)\n",
        "binary_model.train(X_train, y_train, epochs=1000, lr=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2PV0ijEeEXC",
        "outputId": "bf972d6b-d5f7-40c6-da32-9051618abc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000, Loss: 0.0010\n",
            "Epoch 200/1000, Loss: 0.0005\n",
            "Epoch 300/1000, Loss: 0.0003\n",
            "Epoch 400/1000, Loss: 0.0002\n",
            "Epoch 500/1000, Loss: 0.0002\n",
            "Epoch 600/1000, Loss: 0.0002\n",
            "Epoch 700/1000, Loss: 0.0001\n",
            "Epoch 800/1000, Loss: 0.0001\n",
            "Epoch 900/1000, Loss: 0.0001\n",
            "Epoch 1000/1000, Loss: 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "y_pred_test = binary_model.forward(X_test)\n",
        "print(\"Test Loss:\", mean_squared_error(y_test, y_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TJGG6qxeQCr",
        "outputId": "58420fcf-b9b5-4dd9-b9bd-35f1fef22123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 8.575005180900871e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Multiclass Classication"
      ],
      "metadata": {
        "id": "c2KPOdCBeWgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy.special import softmax\n"
      ],
      "metadata": {
        "id": "36F2kVAueYJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dataset using make_blobs for multiclass classification\n",
        "X, y = make_blobs(n_samples=300, centers=3, n_features=4, random_state=42)\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False) # Updated argument\n",
        "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "u2US7KCZeks_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(z):\n",
        " return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Derivative of sigmoid for backpropagation\n",
        "def sigmoid_derivative(z):\n",
        " return sigmoid(z) * (1 - sigmoid(z))"
      ],
      "metadata": {
        "id": "uopCtnKVer_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "def initialize_parameters(input_size, hidden1_size, hidden2_size, output_size):\n",
        "  np.random.seed(42)\n",
        "  W1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
        "  b1 = np.zeros((1, hidden1_size))\n",
        "  W2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
        "  b2 = np.zeros((1, hidden2_size))\n",
        "  W3 = np.random.randn(hidden2_size, output_size) * 0.01\n",
        "  b3 = np.zeros((1, output_size))\n",
        "  return W1, b1, W2, b2, W3, b3"
      ],
      "metadata": {
        "id": "CRdBUG5jirQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation\n",
        "def forward_propagation(X, W1, b1, W2, b2, W3, b3):\n",
        " Z1 = np.dot(X, W1) + b1\n",
        " A1 = sigmoid(Z1)\n",
        " Z2 = np.dot(A1, W2) + b2\n",
        " A2 = sigmoid(Z2)\n",
        " Z3 = np.dot(A2, W3) + b3\n",
        " A3 = softmax(Z3, axis=1)\n",
        " return Z1, A1, Z2, A2, Z3, A3"
      ],
      "metadata": {
        "id": "G9tbs2iVi8ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute categorical cross-entropy loss\n",
        "def compute_loss(y_true, y_pred):\n",
        " m = y_true.shape[0]\n",
        " loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m"
      ],
      "metadata": {
        "id": "WfQr06lCjHfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "def backpropagation(X, y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3):\n",
        " m = X.shape[0]\n",
        " dZ3 = A3 - y\n",
        " dW3 = np.dot(A2.T, dZ3) / m\n",
        " db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
        " dZ2 = np.dot(dZ3, W3.T) * sigmoid_derivative(Z2)\n",
        " dW2 = np.dot(A1.T, dZ2) / m\n",
        " db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        " dZ1 = np.dot(dZ2, W2.T) * sigmoid_derivative(Z1)\n",
        " dW1 = np.dot(X.T, dZ1) / m\n",
        " db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        " return dW1, db1, dW2, db2, dW3, db3\n"
      ],
      "metadata": {
        "id": "Jk8i28lojNWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update parameters\n",
        "def update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
        " W1 -= learning_rate * dW1\n",
        " b1 -= learning_rate * db1\n",
        " W2 -= learning_rate * dW2\n",
        " b2 -= learning_rate * db2\n",
        " W3 -= learning_rate * dW3\n",
        " b3 -= learning_rate * db3\n",
        " return W1, b1, W2, b2, W3, b3"
      ],
      "metadata": {
        "id": "IKV3KMZljiHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the neural network\n",
        "def train(X, y, input_size, hidden1_size, hidden2_size, output_size, epochs, learning_rate):\n",
        " W1, b1, W2, b2, W3, b3 = initialize_parameters(input_size, hidden1_size, hidden2_size, output_size)\n",
        " for epoch in range(epochs):\n",
        "  Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X, W1, b1, W2, b2, W3, b3)\n",
        "  loss = compute_loss(y, A3)\n",
        "  dW1, db1, dW2, db2, dW3, db3 = backpropagation(X, y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3)\n",
        "  W1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
        " if epoch % 100 == 0:\n",
        "  print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        " return W1, b1, W2, b2, W3, b3\n"
      ],
      "metadata": {
        "id": "1MfZ4p6ijvqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "input_size = 4\n",
        "hidden1_size = 3\n",
        "hidden2_size = 4\n",
        "output_size = 3\n",
        "epochs = 1000\n",
        "learning_rate = 0.1\n",
        "W1, b1, W2, b2, W3, b3 = train(X_train, y_train, input_size, hidden1_size, hidden2_size, output_size, epochs, learning_rate)\n"
      ],
      "metadata": {
        "id": "QAlWsX2ZkJHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "_, _, _, _, _, A3_test = forward_propagation(X_test, W1, b1, W2, b2, W3, b3)\n",
        "test_loss = compute_loss(y_test, A3_test)\n",
        "print(f\"Test Loss: {test_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwiXfVWOkQeX",
        "outputId": "3f5edba4-da69-4af2-f638-0526ca01e518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate accuracy\n",
        "predictions = np.argmax(A3_test, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(predictions == y_true)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuRPWU5jkWKg",
        "outputId": "9ad3ce7e-21c6-4b89-9da0-d1151add764c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 30.00%\n"
          ]
        }
      ]
    }
  ]
}